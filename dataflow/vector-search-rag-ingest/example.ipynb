{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae794e8-7270-47e4-b7c3-e519fed81a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after complete\n",
    "!pip install --upgrade google-cloud-aiplatform\n",
    "!pip install pydantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee08798-e00f-4bf8-8fc8-24e34a42c6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "print(aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad7244-fb89-498e-a913-06e4e862b811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "REGION = \"us-central1\"\n",
    "DATASET_FILE_BUCKET = f\"{PROJECT}-dataset-files-test\"\n",
    "VECTOR_SEARCH_BUCKET = f\"{PROJECT}-vector-search-test\"\n",
    "DOCUMENT_BUCKET = f\"{PROJECT}-documents-test\"\n",
    "DEPLOYED_INDEX_ID = \"my_test_index_deployed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcba4b-1756-4359-9bb8-7243082c2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{DATASET_FILE_BUCKET}\n",
    "!gsutil mb -l us-central1 gs://{VECTOR_SEARCH_BUCKET}\n",
    "!gsutil mb -l us-central1 gs://{DOCUMENT_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ca551-dc10-4a67-98a0-5483eb8830a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile test.txt\n",
    "gs://cloud-samples-data/gen-app-builder/search/arxiv/1012.0841v1.Automated_Query_Learning_with_Wikipedia_and_Genetic_Programming.pdf\n",
    "gs://cloud-samples-data/gen-app-builder/search/arxiv/1406.2538v1.FrameNet_CNL_a_Knowledge_Representation_and_Information_Extraction_Language.pdf\n",
    "gs://cloud-samples-data/gen-app-builder/search/arxiv/1409.2944v2.Collaborative_Deep_Learning_for_Recommender_Systems.pdf\n",
    "gs://cloud-samples-data/gen-app-builder/search/arxiv/1412.3714v2.Feature_Weight_Tuning_for_Recursive_Neural_Networks.pdf\n",
    "gs://cloud-samples-data/gen-app-builder/search/arxiv/1412.5335v7.Ensemble_of_Generative_and_Discriminative_Techniques_for_Sentiment_Analysis_of_Movie_Reviews.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc05cc5-caac-4ddd-a930-e0fbfeb2e000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put together a dataset file with a list of PDF paths \n",
    "DATASET_FILE = f\"gs://{DATASET_FILE_BUCKET}/test.txt\"\n",
    "!gsutil cp test.txt {DATASET_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf693a-f2c2-4c0f-9472-9064fefd4c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look at the dataset file in GCS. This is the format of the txt file the Dataflow pipeline expects\n",
    "!gsutil cat {DATASET_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eb74b-ad02-436e-8889-656356e8dcab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Vector Search Index \n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"test-index\",\n",
    "    contents_delta_uri=f\"gs://{VECTOR_SEARCH_BUCKET}\",\n",
    "    dimensions=512,\n",
    "    approximate_neighbors_count=150,\n",
    "    index_update_method=\"STREAM_UPDATE\",\n",
    "    shard_size=\"SHARD_SIZE_SMALL\"\n",
    ")\n",
    "INDEX_ID = index.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bcc92-0cda-4129-8868-3718dc4b5978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to change the chunk size and overlap\n",
    "print(\n",
    "    f\"Google Cloud Project ID: {PROJECT}\\n\"\n",
    "    f\"Dataset file:            {DATASET_FILE}\\n\"\n",
    "    f\"Vector Search Index:     {INDEX_ID}\\n\"\n",
    "    f\"Document Bucket:         {DOCUMENT_BUCKET}\\n\"\n",
    "    f\"Chunk Size:              1000\\n\"\n",
    "    f\"Chunk Overlap:           250\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c0946-54cd-4b2d-abb0-3f4d53f06423",
   "metadata": {},
   "source": [
    "## Executing the Pipeline\n",
    "Using the above runtime parameters, launch an execution of the pipeline. \n",
    "\n",
    "#### Through the UI\n",
    " 1. Navigate to Dataflow in the GCP Console \n",
    " 1. Select 'Create job from template'\n",
    " 1. Specify a unique job name (e.g. pdf-to-vector-pipeline-run-{timestamp})\n",
    " 1. In Template Select, scroll down to the button and select 'Custom Template'\n",
    " 1. Browse to and select the template path (you can find this as an output of the final code cell in setup.ipynb)\n",
    " 1. Input the above pipeline parameters \n",
    " 1. Select 'Run Job'\n",
    " \n",
    "#### Using gcloud CLI\n",
    "```python\n",
    "!gcloud dataflow flex-template run \"pdf-to-vector-search-`date +%Y%m%d-%H%M%S`\" \\\n",
    " --template-file-gcs-location={TEMPLATE_FILE} \\\n",
    " --region={REGION} \\\n",
    " --num-workers={NUM_WORKERS} \\\n",
    " --max-workers={MAX_WORKERS} \\\n",
    " --worker-machine-type={MACHINE_TYPE} \\\n",
    " --parameters project_id={PROJECT} \\\n",
    " --parameters dataset_file={DATASET_FILE} \\\n",
    " --parameters index_id={INDEX_ID} \\\n",
    " --parameters document_bucket={DOCUMENT_BUCKET} \\\n",
    " --parameters chunk_size={CHUNK_SIZE} \\\n",
    " --parameters chunk_overlap={CHUNK_OVERLAP}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda2ec9-92cf-46a5-881a-ba01eadbc788",
   "metadata": {},
   "source": [
    "While the pipeline to embed the PDFs and ingest into Vector Search is running, create an endpoint to host the index and deploy the index to the endpoint so we can serve nearest neighbors queries for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9d133-dd31-4899-abb4-3713a384c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Search Endpoint \n",
    "endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f'{INDEX_ID}-endpoint',\n",
    "    public_endpoint_enabled=True\n",
    ")\n",
    "ENDPOINT_ID = endpoint.resource_name \n",
    "print(f\"ENDPOINT_ID: {ENDPOINT_ID}\")\n",
    "\n",
    "# Deploy index to endpoint\n",
    "deployed_index = endpoint.deploy_index(\n",
    "    index=index,\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5cef6-cd2a-4c5c-aa22-969b68baf63b",
   "metadata": {},
   "source": [
    "## End to End Retrieval System\n",
    "\n",
    "**NOTE** You need to wait for the pipeline to successfully finish, and the endpoint to be deployed before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4b7d7-889f-4934-8ead-9243be9a1162",
   "metadata": {},
   "source": [
    "Create a hepler class that implements the retrieval chain for a research paper chat agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118943b3-edbd-4e69-b183-4d27d7d31885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import json \n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel, \n",
    "    GenerationConfig,\n",
    "    GenerationResponse\n",
    ")\n",
    "from google.cloud import storage\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class DocumentChunk(BaseModel):\n",
    "    source_file: str \n",
    "    page_number: int \n",
    "    content: str \n",
    "    \n",
    "class Response(BaseModel):\n",
    "    text: str | None = None\n",
    "    full_response: dict \n",
    "    citations: List[DocumentChunk] | None = None \n",
    "    \n",
    "class ResearchPaperChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint_id: str,\n",
    "        deployed_index_id: str,\n",
    "        document_bucket: str,\n",
    "        k: int,\n",
    "        model_id: str = \"gemini-1.5-pro-001\",\n",
    "        temperature: float = 0.2,\n",
    "        max_output_tokens: int = 2500,\n",
    "        sys_msg: str = \"\"\"\n",
    "        You are a friendly research assistant with access to many research papers. \n",
    "        You only answer questions about research. You do not make up any new facts.\n",
    "        \"\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        endpoint_id: str. Vertex AI Vector Search index endpoint ID. \n",
    "        deployed_index_id: str. Deployed Index ID on Vector Search endpoint.\n",
    "        k: int. Number of nearest neighbors to include in retrieval.\n",
    "        model_id: str. Underlying LLM to use. Must be Gemini variant.\n",
    "        sys_msg: str. System message to initialize model with.\n",
    "        document_bucket: str. GCS bucket name where {vector_id}.json files are stored. \n",
    "                    Each file needs to have keys: {source_file, page_number, content}\n",
    "        \n",
    "        \"\"\"        \n",
    "        self.k = k \n",
    "        # Load embedding model that was used to create vector database \n",
    "        self.embedding_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        self.endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)\n",
    "        self.bucket = storage.Client().get_bucket(document_bucket) \n",
    "        self.deployed_index_id = deployed_index_id \n",
    "        \n",
    "        model = GenerativeModel(\n",
    "            model_id,\n",
    "            system_instruction=sys_msg,\n",
    "            generation_config=GenerationConfig(\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens\n",
    "            )\n",
    "        )\n",
    "        self.chat_session = model.start_chat()\n",
    "        \n",
    "    def send_message(self, message: str) -> Response:\n",
    "        \n",
    "        query_embedding = self._get_embedding(message) # Embed query \n",
    "        query_neighbor_ids = self._find_neighbors(query_embedding) # Find nearest neighbor IDs\n",
    "        \n",
    "        # Read documents for nearest neighbors \n",
    "        neighbor_docs = [\n",
    "            self._get_document(doc_id) \n",
    "            for doc_id in query_neighbor_ids\n",
    "        ]\n",
    "        \n",
    "        # Put nearest neighbors content in a prompt with the query \n",
    "        full_message = self._get_prompt(\n",
    "            query=message,\n",
    "            documents=neighbor_docs\n",
    "        )   \n",
    "        response = self.chat_session.send_message(full_message)\n",
    "        try:\n",
    "            response_text = response.text \n",
    "        except:\n",
    "            response_text = None \n",
    "            \n",
    "        return Response(\n",
    "            text=response_text,\n",
    "            full_response=response.to_dict(),\n",
    "            citations=neighbor_docs\n",
    "        )\n",
    "        \n",
    "    def _get_embedding(self, text: str) -> List[float]: \n",
    "        output = self.embedding_model([text])\n",
    "        return tf.squeeze(output).numpy().tolist()\n",
    "    \n",
    "    def _find_neighbors(self, vector: List[float]) -> List[str]: \n",
    "        neighbors = self.endpoint.find_neighbors(\n",
    "            deployed_index_id=self.deployed_index_id,\n",
    "            queries=[\n",
    "                vector\n",
    "            ],\n",
    "            num_neighbors=self.k\n",
    "        )\n",
    "        \n",
    "        # Dont fail if there are no neighbors found. Simply return no context data points \n",
    "        if not neighbors:\n",
    "            return [] \n",
    "        \n",
    "        return [x.id for x in neighbors[0]]\n",
    "    \n",
    "    def _get_document(self, doc_id: str) -> DocumentChunk: \n",
    "        blob = self.bucket.blob(f'documents/{doc_id}.json')\n",
    "        data = json.loads(blob.download_as_string())\n",
    "        return DocumentChunk(**data) \n",
    "    \n",
    "    def _get_prompt(self, query: str, documents: List[DocumentChunk] | None = None) -> str:\n",
    "        content_list = [\n",
    "            f\"\\nSource: {d.source_file}\\nPage Num: {d.page_number}\\nContent: {d.content}\\n\\n\"\n",
    "            for d in documents \n",
    "        ]\n",
    "        content_str = '\\n'.join(content_list)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Using only the provided context from research papers, answer the question.\n",
    "        If you cannot answer the question using only the provided context, \n",
    "        respond that you do not have the context needed to answer the question.\n",
    "        \n",
    "        Question: {query}.\n",
    "        \n",
    "        Context:\n",
    "        {content_str}\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "def pretty_print_response(response: Response, include_content: bool = False):\n",
    "    print(response.text)\n",
    "    print(\"\\nCitations:\\n\")\n",
    "    for c in response.citations: \n",
    "        print(f\"\\nSource PDF: {c.source_file}\\nPage Number: {c.page_number}\\n\")\n",
    "        if include_content:\n",
    "            print(f\"{c.content} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5ef21-d88b-4f03-a699-d395bfd15714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If session was interupted then be sure to initialize the needed variables \n",
    "# ENDPOINT_ID = \"{YOUR VECTOR SEARCH ENDPOINT ID}\"\n",
    "# DEPLOYED_INDEX_ID = \"my_test_index_deployed\"\n",
    "# PROJECT = !(gcloud config get-value core/project)\n",
    "# PROJECT = PROJECT[0]\n",
    "# DOCUMENT_BUCKET = f\"{PROJECT}-documents-test\"\n",
    "\n",
    "chat = ResearchPaperChat(\n",
    "    endpoint_id=ENDPOINT_ID,\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    document_bucket=DOCUMENT_BUCKET,\n",
    "    k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1117a9-4170-4cb0-bd74-13763466222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"What is sentence pair scoring?\")\n",
    "pretty_print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e6e25-3f0b-471b-9f6d-55f9ce64c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Interesting. What is batch softmax contrastive loss?\")\n",
    "pretty_print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2963d1-316e-4cb1-b6a1-184045028aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
